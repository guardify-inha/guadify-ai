# scripts/query_and_extract.py
import os, re, json, argparse, pickle
import openai
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from datetime import datetime

# ----------------- ì„¤ì • -----------------
OPENAI_MODEL = "gpt-4o"
OPENAI_MAX_TOKENS = 1500
OPENAI_TEMPERATURE = 0.0

INDEX_FILE = "../outputs/faiss.index"
META_FILE = "../outputs/faiss_meta.pkl"
EMBED_MODEL = "all-MiniLM-L6-v2"

MAX_PROMPT_CHARS = 15000
SIMILARITY_THRESHOLD = 0.4   # í‘œì¤€ì•½ê´€ê³¼ "ë¹„ìŠ·í•˜ë‹¤"ê³  ë³´ëŠ” ê¸°ì¤€ (ê±°ë¦¬ ê¸°ì¤€, ì¡°ì • í•„ìš”)

ARTICLE_RE = re.compile(r'(ì œ\s*\d+\s*ì¡°[^\n]*)')

# ----------------- ì¡°í•­ ë‹¨ìœ„ ë¶„ë¦¬ -----------------
def split_by_article(text):
    matches = list(ARTICLE_RE.finditer(text))
    if not matches:
        return [{"article": None, "text": text.strip()}]

    results = []
    for i, m in enumerate(matches):
        start_idx = m.start()
        end_idx = matches[i+1].start() if i+1 < len(matches) else len(text)
        article_title = m.group(1).strip()
        body = text[start_idx:end_idx].strip()
        results.append({"article": article_title, "text": body})
    return results

# ----------------- ë²¡í„° ê²€ìƒ‰ -----------------
embed_model = SentenceTransformer(EMBED_MODEL)

def load_faiss():
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "rb") as f:
        meta = pickle.load(f)
    return index, meta

def search_standard_similarity(query_text, top_k=1):
    """í‘œì¤€ì•½ê´€(source_tag == 'standard') ì¤‘ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸°"""
    index, meta = load_faiss()
    q_emb = embed_model.encode([query_text]).astype("float32")
    D, I = index.search(q_emb, top_k)

    results = []
    for dist, idx in zip(D[0], I[0]):
        if idx < 0:
            continue
        rec = meta[idx]
        if rec.get("source_tag") != "standard":
            continue
        results.append({"text": rec["text"], "score": float(dist)})
    return results

# ----------------- LLM í˜¸ì¶œ -----------------
def call_openai_filter(chunks, api_key=None):
    """ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ LLMì—ê²Œ ì ì¬ì  ìœ„í—˜ í›„ë³´ë¥¼ ìµœëŒ€í•œ ë°˜í™˜"""
    if api_key is None:
        api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    client = openai.OpenAI(api_key=api_key)

    assembled = []
    total_len = 0
    for i, c in enumerate(chunks):
        piece = f"[chunk{i}] ({c['article']}) {c['text']}"
        if total_len + len(piece) > MAX_PROMPT_CHARS:
            break
        assembled.append(piece)
        total_len += len(piece)

    system_msg = (
        "You are a Korean legal assistant. "
        "Inputì€ ê³„ì•½ ì¡°í•­ í›„ë³´ì…ë‹ˆë‹¤.\n"
        "ëª¨ë“  ì¡°í•­ ì¤‘ ì ì¬ì  ìœ„í—˜ì´ ìˆëŠ” ì¡°í•­ì€ ìµœëŒ€í•œ í¬í•¨í•˜ì„¸ìš”. "
        "ê³µì •í•œ ì¡°í•­ì´ë¼ë„ ìœ„í—˜ ê°€ëŠ¥ì„±ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆìœ¼ë©´ í›„ë³´ë¡œ ë‚¨ê²¨ì•¼ í•©ë‹ˆë‹¤.\n"
        "Return ONLY valid JSON array. Each element must have:\n"
        " - article: related article title\n"
        " - excerpt: short excerpt (<=300 chars)\n"
        " - reason: why it may be unfair\n"
    )

    user_msg = "Analyze the following clauses. Include all clauses with any potential risk, even if minor:\n\n" + "\n\n---\n\n".join(assembled)

    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[{"role": "system", "content": system_msg},
                  {"role": "user", "content": user_msg}],
        temperature=OPENAI_TEMPERATURE,
        max_tokens=OPENAI_MAX_TOKENS
    )
    text = resp.choices[0].message.content.strip()

    try:
        return json.loads(text)
    except Exception:
        m = re.search(r"(\[.*\])", text, flags=re.S)
        if m:
            try:
                return json.loads(m.group(1))
            except Exception:
                return {"error": "failed_to_parse_json", "raw": text}
        return {"error": "no_json_found", "raw": text}

# ----------------- ë©”ì¸ ë¡œì§ -----------------
def analyze_contract(text, openai_call=True, output_dir="../outputs"):
    # 1. ê³„ì•½ì„œì—ì„œ ì¡°í•­ ë‹¨ìœ„ ë¶„ë¦¬
    clauses = split_by_article(text)

    # 2. í‘œì¤€ì•½ê´€ê³¼ ìœ ì‚¬í•œ ê±´ ì œì™¸
    filtered = []
    for c in clauses:
        sims = search_standard_similarity(c["text"], top_k=1)
        if sims and sims[0]["score"] < SIMILARITY_THRESHOLD:
            continue
        filtered.append(c)

    # 3. LLMìœ¼ë¡œ ì ì¬ì  ìœ„í—˜ í›„ë³´ ìµœëŒ€í•œ ì¶”ì¶œ
    llm_result = None
    llm_filtered_count = 0
    if openai_call and filtered:
        llm_result = call_openai_filter(filtered)
        if isinstance(llm_result, list):
            llm_filtered_count = len(filtered) - len(llm_result)

    # íƒ€ì„ìŠ¤íƒ¬í”„ í´ë” ìƒì„± (ì´ˆë‹¨ìœ„ê¹Œì§€)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = os.path.join(output_dir, f"analysis_{timestamp}")
    os.makedirs(result_dir, exist_ok=True)
    
    result = {
        "timestamp": datetime.now().isoformat(),
        "total_articles": len(clauses),
        "after_standard_filter": len(filtered),
        "llm_filtered_count": llm_filtered_count,
        "final_candidates": llm_result
    }

    # ì €ì¥
    save_path = os.path.join(result_dir, "query_results.json")
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {result_dir}")

    # ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
    print("==== ë¶„ì„ ê²°ê³¼ ====")
    print(f"ì´ ì¡°í•­ ìˆ˜: {len(clauses)}")
    print(f"í‘œì¤€ì•½ê´€ í•„í„° ì´í›„ ë‚¨ì€ ì¡°í•­: {len(filtered)}")
    print(f"LLMìœ¼ë¡œ ì œì™¸ëœ ì¡°í•­ ìˆ˜: {llm_filtered_count}")
    print("ì ì¬ì  ìœ„í—˜ ì¡°í•­:")
    if isinstance(llm_result, list):
        for r in llm_result:
            print(f"- {r.get('article')}: {r.get('excerpt')} ({r.get('reason')})")
    else:
        print(llm_result)

    return result

# ----------------- ì‹¤í–‰ìš© CLI -----------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--file", help="ë¶„ì„í•  ê³„ì•½ì„œ(í…ìŠ¤íŠ¸) íŒŒì¼ ê²½ë¡œ", required=True)
    parser.add_argument("--openai", action="store_true", help="OpenAI í˜¸ì¶œ ì—¬ë¶€")
    args = parser.parse_args()

    with open(args.file, "r", encoding="utf-8") as f:
        text = f.read()

    analyze_contract(text, openai_call=args.openai)
