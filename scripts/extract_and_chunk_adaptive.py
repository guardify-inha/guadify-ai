"""
ì ì‘í˜• ì²­í‚¹ì„ ì‚¬ìš©í•œ ê°œì„ ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ extract_and_chunk.pyë¥¼ ì ì‘í˜• ì²­í‚¹ ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë„ë¡ ê°œì„ í•œ ë²„ì „ì…ë‹ˆë‹¤.
"""

import os
import json
import fitz  # pymupdf
from tqdm import tqdm
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.chunking import chunk_document, DocumentType

# ë°ì´í„° ë£¨íŠ¸ (ìƒëŒ€ê²½ë¡œ)
DATA_ROOT = "data"
# ì²˜ë¦¬í•  í•˜ìœ„ í´ë”ë“¤
CONTRACTS_DIR = os.path.join(DATA_ROOT, "contracts", "reference")
LAW_DIR = os.path.join(DATA_ROOT, "law")
STANDARD_DIR = os.path.join(DATA_ROOT, "standard")
PRECEDENT_DIR = os.path.join(DATA_ROOT, "precedent")  # ìƒˆë¡œ ì¶”ê°€

OUT_FILE = "outputs/chunks_adaptive.jsonl"
MAX_PAGE_CHARS = 200_000   # í˜ì´ì§€ë‹¹ ìµœëŒ€ ê¸€ì ìˆ˜ ì œí•œ

def process_pdf_file_adaptive(path, source_tag):
    """
    ì ì‘í˜• ì²­í‚¹ì„ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        path: PDF íŒŒì¼ ê²½ë¡œ
        source_tag: ë¬¸ì„œ ìœ í˜• ('contract', 'law', 'standard', 'precedent')
    """
    doc = fitz.open(path)
    fname = os.path.basename(path)
    
    print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {fname} (ìœ í˜•: {source_tag})")
    
    for page_num, page in enumerate(doc, start=1):
        txt = page.get_text()
        if not txt:
            continue

        # ë„ˆë¬´ í° í˜ì´ì§€ëŠ” truncate ì²˜ë¦¬
        if len(txt) > MAX_PAGE_CHARS:
            print(f"âš ï¸  í° í˜ì´ì§€ {page_num} ì˜ë¼ë‚´ê¸°: {fname} ({len(txt)} ë¬¸ì)")
            txt = txt[:MAX_PAGE_CHARS]

        # ì ì‘í˜• ì²­í‚¹ ì ìš©
        try:
            chunks = chunk_document(
                text=txt,
                document_type=source_tag,
                source_file=fname,
                page=page_num
            )
            
            for chunk in chunks:
                # ê¸°ì¡´ í˜•ì‹ê³¼ í˜¸í™˜ë˜ë„ë¡ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                chunk_data = {
                    "chunk_id": chunk["chunk_id"],
                    "source_file": chunk["source_file"],
                    "source_tag": source_tag,
                    "page": chunk["page"],
                    "chunk_idx": chunk.get("chunk_identifier", "unknown"),
                    "text": chunk["text"],
                    "length": chunk["length"],
                    "chunking_strategy": "adaptive"  # ìƒˆë¡œìš´ í•„ë“œ
                }
                yield chunk_data
                
        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì˜¤ë¥˜ {fname} í˜ì´ì§€ {page_num}: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì²­í‚¹ìœ¼ë¡œ í´ë°±
            yield {
                "chunk_id": str(uuid.uuid4()),
                "source_file": fname,
                "source_tag": source_tag,
                "page": page_num,
                "chunk_idx": 0,
                "text": txt[:1000],  # ì²« 1000ìë§Œ
                "length": min(len(txt), 1000),
                "chunking_strategy": "fallback"
            }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import uuid
    
    os.makedirs("outputs", exist_ok=True)
    out_path = OUT_FILE

    # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    file_list = []
    
    # ê³„ì•½ì„œ
    if os.path.isdir(CONTRACTS_DIR):
        for f in os.listdir(CONTRACTS_DIR):
            if f.lower().endswith(".pdf"):
                file_list.append((os.path.join(CONTRACTS_DIR, f), "contract"))
    
    # ë²•ë¥  (temp ë””ë ‰í† ë¦¬ ì œì™¸)
    if os.path.isdir(LAW_DIR):
        for f in os.listdir(LAW_DIR):
            if f.lower().endswith(".pdf") and f != "temp":
                file_list.append((os.path.join(LAW_DIR, f), "law"))
    
    # í‘œì¤€
    if os.path.isdir(STANDARD_DIR):
        for f in os.listdir(STANDARD_DIR):
            if f.lower().endswith(".pdf"):
                file_list.append((os.path.join(STANDARD_DIR, f), "standard"))
    
    # íŒë¡€ (ìƒˆë¡œ ì¶”ê°€)
    if os.path.isdir(PRECEDENT_DIR):
        for f in os.listdir(PRECEDENT_DIR):
            if f.lower().endswith(".pdf"):
                file_list.append((os.path.join(PRECEDENT_DIR, f), "precedent"))

    if not file_list:
        print("âŒ ì„¤ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   í™•ì¸í•  ë””ë ‰í† ë¦¬: {DATA_ROOT}")
        return

    print(f"ğŸš€ {len(file_list)}ê°œ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
    print("ğŸ“Š ì ì‘í˜• ì²­í‚¹ ì „ëµ ì ìš©:")
    print("   - ê³„ì•½ì„œ: ì¡°í•­ ë‹¨ìœ„ + ë¬¸ì¥ ê²½ê³„ ê³ ë ¤")
    print("   - ë²•ë¥ : ì¡°í•­ ë‹¨ìœ„ + í•˜ìœ„ ë‹¨ìœ„(í•­, í˜¸) ê³ ë ¤")
    print("   - í‘œì¤€: ì¡°í•­ ë‹¨ìœ„")
    print("   - íŒë¡€: ë¬¸ì¥ ë‹¨ìœ„ + ì˜ë¯¸ì  ê²½ê³„ ê³ ë ¤")

    total_chunks = 0
    with open(out_path, "w", encoding="utf-8") as outf:
        for path, tag in tqdm(file_list, desc="ğŸ“„ PDF ì²˜ë¦¬"):
            try:
                for rec in process_pdf_file_adaptive(path, tag):
                    outf.write(json.dumps(rec, ensure_ascii=False) + "\n")
                    total_chunks += 1
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {path}: {e}")

    print(f"âœ… ì™„ë£Œ! ì²­í¬ íŒŒì¼ ì €ì¥: {out_path}")
    print(f"ğŸ“Š ì´ {total_chunks}ê°œ ì²­í¬ ìƒì„±")

if __name__ == "__main__":
    main()
