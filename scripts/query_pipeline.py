# scripts/query_pipeline.py
import os, json, pickle, sys
import faiss, numpy as np
from sentence_transformers import SentenceTransformer
import openai

# ----------------- ì„¤ì • -----------------
INDEX_FILE = "../outputs/faiss.index"
META_FILE = "../outputs/faiss_meta.pkl"
EMBED_MODEL = "all-MiniLM-L6-v2"
CANDIDATE_MAX = 20

LAW_DIR = "../law"
TEMP_DIR = "../temp"
REFERENCE_FILE = "../data/outputs/chunks.jsonl"  # <- ë³€ê²½

# ----------------- FAISS & ëª¨ë¸ ë¡œë“œ -----------------
index = faiss.read_index(INDEX_FILE)
with open(META_FILE, "rb") as f:
    meta = pickle.load(f)
model = SentenceTransformer(EMBED_MODEL)

# ----------------- OpenAI -----------------
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_KEY:
    print("Error: OPENAI_API_KEY not set")
    sys.exit(1)
openai.api_key = OPENAI_KEY

# ----------------- reference ì˜ˆì‹œ ë¡œë“œ (chunks.jsonl) -----------------
def load_reference_examples():
    examples = []
    if not os.path.exists(REFERENCE_FILE):
        print(f"Warning: {REFERENCE_FILE} not found")
        return examples
    with open(REFERENCE_FILE, "r", encoding="utf-8") as f:
        for idx, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            if obj.get("source_tag") == "contract":
                examples.append({
                    "source_file": obj.get("source_file", "unknown"),
                    "chunk_idx": idx,
                    "text": obj.get("text", "")
                })
    return examples

REFERENCE_EXAMPLES = load_reference_examples()

# ----------------- ìœ ì‚¬ë„ ê²€ìƒ‰ -----------------
def retrieve_similar(text, top_k=5):
    emb = model.encode([text]).astype("float32")
    D, I = index.search(emb, top_k)
    results = []
    for idx in I[0]:
        results.append(meta[idx])
    return results

# ----------------- í›„ë³´ ì¡°í•­ ì¶”ì¶œ -----------------
def extract_candidates_llm(full_text, max_candidates=CANDIDATE_MAX):
    prompt = f"""
You are a Korean legal assistant.
From the following contract text, extract up to {max_candidates} clauses that may be unfair to consumers.
Return as valid JSON array with objects like: {{"clause": "..."}}
Make sure the output is valid JSON parsable by Python json.loads.

Contract text:
\"\"\"{full_text}\"\"\""""
    
    resp = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
        max_tokens=1000
    )
    text = resp.choices[0].message.content.strip()
    # ì½”ë“œ ë¸”ë¡ ì œê±°
    if text.startswith("```"):
        text = "\n".join(text.split("\n")[1:])
        if text.endswith("```"):
            text = "\n".join(text.split("\n")[:-1])
    text = text.strip()

    try:
        data = json.loads(text)
        return [item["clause"] for item in data if "clause" in item]
    except Exception as e:
        print("Error parsing JSON from LLM:", e)
        print("LLM raw response:", text)
        return []

# ----------------- ìµœì¢… ìœ„í—˜ íŒë‹¨ -----------------
def ask_llm_for_risk_clause(clause_text, retrieved_examples, reference_examples):
    prompt = f"""
You are a legal assistant. Evaluate the risk that the following contract clause is an unfair term.

Clause:
\"\"\"{clause_text}\"\"\" 

Consider the following:
1. Prior law/guidelines in order of priority: 
   - ì•½ê´€ì‹¬ì‚¬ì§€ì¹¨.pdf
   - Other laws in {LAW_DIR}
   - ì‹œí–‰ë ¹ in {TEMP_DIR} (today focus on ê¸ˆìœµ)
2. Previously judged unfair examples from reference folder:
"""
    for ex in reference_examples:
        if ex['text'] in clause_text:
            prompt += f"- Match with {ex['source_file']} line {ex['chunk_idx']}: {ex['text'][:150]}...\n"

    prompt += """
Answer in valid JSON with:
{
  "risk_percent": int(0-100),
  "risk_level": "low/medium/high",
  "reason": "...",
  "suggested_law_codes": ["..."]
}
Use only the provided data. Do not hallucinate.
"""
    resp = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"user","content":prompt}],
        temperature=0.0,
        max_tokens=400
    )
    text = resp.choices[0].message.content.strip()
    # ì½”ë“œ ë¸”ë¡ ì œê±°
    if text.startswith("```"):
        text = "\n".join(text.split("\n")[1:])
        if text.endswith("```"):
            text = "\n".join(text.split("\n")[:-1])
    text = text.strip()

    try:
        return json.loads(text)
    except Exception:
        print("Warning: LLM returned non-JSON. Raw:", text)
        return {"risk_percent":0, "risk_level":"low", "reason":"failed parsing","suggested_law_codes":[]}

# ----------------- ë©”ì¸ -----------------
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python query_pipeline.py path/to/query_results.json")
        sys.exit(1)

    query_results_path = sys.argv[1]
    results = json.load(open(query_results_path, encoding="utf-8"))
    final_candidates = results.get("final_candidates", [])

    if not final_candidates:
        print("No candidate clauses found.")
        sys.exit(0)

    all_results = []
    summary_lines = []

    for c in final_candidates:
        clause_text = c.get("excerpt") or c.get("text") or c
        clause_identifier = c.get("law_article") or c.get("article") or "Unknown Article"
        retrieved = retrieve_similar(clause_text, top_k=5)
        llm_response = ask_llm_for_risk_clause(clause_text, retrieved, REFERENCE_EXAMPLES)
        
        # 1) JSON í˜•íƒœë¡œ ì €ìž¥
        record = {
            "article": clause_identifier,
            "clause_text": clause_text,
            "risk_eval": llm_response
        }
        all_results.append(record)

        # 2) ì‚¬ëžŒì´ ì½ê¸° ì¢‹ì€ í…ìŠ¤íŠ¸ ìš”ì•½
        summary_lines.append(
            f"ì¡°í•­: {clause_identifier}\n"
            f"ë¬¸ìž¥ ì›ë¬¸:\n{clause_text}\n"
            f"ë¦¬ìŠ¤í¬: {llm_response.get('risk_percent', 0)}% / {llm_response.get('risk_level','low')}\n"
            f"ì°¸ê³  ë²•/íŒë¡€: {', '.join(llm_response.get('suggested_law_codes', []))}\n"
            f"ì„¤ëª…: {llm_response.get('reason','')}\n"
            f"{'-'*80}\n"
        )

    # query_results.jsonê³¼ ê°™ì€ í´ë”ì— ì €ìž¥
    result_dir = os.path.dirname(query_results_path)
    
    # ì „ì²´ JSON ì €ìž¥
    with open(os.path.join(result_dir, "all_results.json"), "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # ì‚¬ëžŒì´ ì½ê¸° ì¢‹ì€ ìš”ì•½ ì €ìž¥
    with open(os.path.join(result_dir, "summary.txt"), "w", encoding="utf-8") as f:
        f.writelines(summary_lines)

    print(f"ðŸ“ ê²°ê³¼ ì €ìž¥ ìœ„ì¹˜: {result_dir}")
    print("Evaluation done. JSON saved to all_results.json, summary saved to summary.txt")
