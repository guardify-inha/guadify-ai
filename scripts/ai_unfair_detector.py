"""
AI ê¸°ë°˜ ë¶ˆê³µì • ì•½ê´€ íƒì§€ ì‹œìŠ¤í…œ

LLM í•¸ë“¤ëŸ¬ë¥¼ í™œìš©í•œ ë§¥ë½ ê¸°ë°˜ ë¶ˆê³µì • ì¡°í•­ íƒì§€
"""

# Windows í™˜ê²½ì—ì„œ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
import sys
import os
if sys.platform.startswith('win'):
    import locale
    import codecs
    # UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì •
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    os.environ['LANG'] = 'ko_KR.UTF-8'

import json
import logging
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from models.llm_handler import LLMHandler, LegalContext, UnfairClauseAnalysis
from db.neo4j_client import Neo4jClient
from db.graph_retriever import GraphRetriever
from db.entity_extractor import LegalEntityExtractor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class DetectionResult:
    """íƒì§€ ê²°ê³¼"""
    clause_text: str
    analysis: UnfairClauseAnalysis
    legal_context: List[LegalContext]
    clause_index: int

class AIUnfairDetector:
    """AI ê¸°ë°˜ ë¶ˆê³µì • ì•½ê´€ íƒì§€ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.llm_handler = LLMHandler()
        self.neo4j_client = Neo4jClient()
        self.graph_retriever = GraphRetriever(self.neo4j_client)
        self.entity_extractor = LegalEntityExtractor()
        
        # ë¬¸ì„œ íƒ€ì…ë³„ ìš°ì„ ìˆœìœ„
        self.document_priority = {
            "law": {"weight": 1.0, "description": "ë²•ë ¹"},
            "standard": {"weight": 0.8, "description": "í‘œì¤€ì•½ê´€/ì§€ì¹¨"},
            "reference": {"weight": 0.3, "description": "ë³´ë„ìë£Œ/ì‚¬ë¡€"}
        }
        
        logger.info("AI ê¸°ë°˜ ë¶ˆê³µì • ì•½ê´€ íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def detect_unfair_clauses(self, contract_text: str) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ë¶ˆê³µì • ì¡°í•­ íƒì§€"""
        logger.info(f"ğŸ¤– AI ê¸°ë°˜ ë¶ˆê³µì • ì¡°í•­ íƒì§€ ì‹œì‘: {len(contract_text)}ì")
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self._split_into_sentences(contract_text)
        
        detection_results = []
        total_risk_score = 0
        unfair_count = 0
        
        for i, sentence in enumerate(sentences):
            if len(sentence.strip()) < 10:
                continue
                
            logger.info(f"ì¡°í•­ {i+1}/{len(sentences)} ë¶„ì„ ì¤‘...")
            
            # 1. ë²•ì  ë§¥ë½ ìˆ˜ì§‘
            legal_context = self._collect_legal_context(sentence)
            
            # 2. AI ë¶„ì„
            analysis = self.llm_handler.analyze_clause(sentence, legal_context)
            
            # 3. ê²°ê³¼ ì €ì¥
            result = DetectionResult(
                clause_text=sentence,
                analysis=analysis,
                legal_context=legal_context,
                clause_index=i
            )
            detection_results.append(result)
            
            if analysis.is_unfair:
                unfair_count += 1
                total_risk_score += analysis.confidence
        
        # ì „ì²´ ìœ„í—˜ë„ ê³„ì‚°
        overall_risk = total_risk_score / unfair_count if unfair_count > 0 else 0
        
        return {
            "input_text": contract_text,
            "detection_results": detection_results,
            "unfair_count": unfair_count,
            "total_clauses": len(detection_results),
            "overall_risk_score": overall_risk,
            "risk_level": self._get_risk_level(overall_risk),
            "summary": self._generate_summary(detection_results, overall_risk)
        }
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        clauses = []
        current_clause = ""
        
        for line in text.split('\n'):
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('ì œ') and 'ì¡°' in line:
                if current_clause:
                    clauses.append(current_clause)
                current_clause = line
            else:
                current_clause += " " + line
        
        if current_clause:
            clauses.append(current_clause)
        
        return clauses
    
    def _collect_legal_context(self, sentence: str) -> List[LegalContext]:
        """Graph RAGë¡œ ë²•ì  ë§¥ë½ ìˆ˜ì§‘"""
        legal_context = []
        
        try:
            # ì—”í‹°í‹° ì¶”ì¶œ
            entities = self.entity_extractor.extract_entities(sentence, "input", "clause")
            
            for entity in entities:
                if entity.entity_type.value in ["Article", "Law"]:
                    # Graph RAG ê²€ìƒ‰
                    search_results = self.graph_retriever.search(entity.text, max_results=3)
                    
                    for result in search_results:
                        doc_type = self._get_document_type(result.source_file)
                        context = LegalContext(
                            content=result.context,
                            source=result.source_file,
                            document_type=doc_type,
                            confidence=result.confidence,
                            relevance_score=self._calculate_relevance(sentence, result.entity_text)
                        )
                        legal_context.append(context)
            
            # ê°€ì¤‘ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            legal_context.sort(key=lambda x: x.confidence * self.document_priority[x.document_type]["weight"], reverse=True)
            
        except Exception as e:
            logger.error(f"ë²•ì  ë§¥ë½ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return legal_context[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
    
    def _get_document_type(self, source_file: str) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ë¬¸ì„œ íƒ€ì… ì¶”ì¶œ"""
        if "law/" in source_file:
            return "law"
        elif "standard/" in source_file:
            return "standard"
        elif "reference/" in source_file:
            return "reference"
        return "reference"  # ê¸°ë³¸ê°’
    
    def _calculate_relevance(self, sentence: str, entity_text: str) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        relevance = 0.0
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        sentence_lower = sentence.lower()
        entity_lower = entity_text.lower()
        
        if any(keyword in sentence_lower for keyword in entity_lower.split()):
            relevance += 0.5
        
        # ë²•ì  ê°œë… ê´€ë ¨ì„±
        legal_keywords = ["ë¶ˆê³µì •", "ì•½ê´€", "ì†Œë¹„ì", "ê¶Œë¦¬", "ë³´í˜¸", "ì†í•´", "ë°°ìƒ", "í•´ì§€"]
        if any(keyword in sentence_lower for keyword in legal_keywords):
            relevance += 0.3
        
        return min(relevance, 1.0)
    
    def _get_risk_level(self, score: float) -> str:
        """ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •"""
        if score >= 0.8:
            return "ë§¤ìš°ë†’ìŒ"
        elif score >= 0.6:
            return "ë†’ìŒ"
        elif score >= 0.4:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"
    
    def _generate_summary(self, results: List[DetectionResult], overall_risk: float) -> str:
        """ìš”ì•½ ìƒì„±"""
        unfair_count = sum(1 for r in results if r.analysis.is_unfair)
        high_confidence_count = sum(1 for r in results if r.analysis.is_unfair and r.analysis.confidence >= 0.7)
        
        summary = f"ì´ {len(results)}ê°œ ì¡°í•­ ì¤‘ {unfair_count}ê°œì˜ ë¶ˆê³µì • ì¡°í•­ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
        summary += f"ì´ ì¤‘ {high_confidence_count}ê°œê°€ ë†’ì€ ì‹ ë¢°ë„(â‰¥0.7)ë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤. "
        summary += f"ì „ì²´ ìœ„í—˜ë„ëŠ” {overall_risk:.2f}ì…ë‹ˆë‹¤."
        
        return summary
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.neo4j_client.close()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AI ê¸°ë°˜ ë¶ˆê³µì • ì•½ê´€ íƒì§€')
    parser.add_argument('--file', help='ë¶„ì„í•  ì•½ê´€ íŒŒì¼ ê²½ë¡œ', default='test_contract_mixed.txt')
    parser.add_argument('--output', help='ê²°ê³¼ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # íŒŒì¼ ì½ê¸°
    if not os.path.exists(args.file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.file}")
        return
    
    with open(args.file, 'r', encoding='utf-8') as f:
        contract_text = f.read()
    
    # íƒì§€ ì‹¤í–‰
    detector = AIUnfairDetector()
    result = detector.detect_unfair_clauses(contract_text)
    detector.close()
    
    # ê²°ê³¼ ì¶œë ¥
    print("=" * 80)
    print("ğŸ¤– AI ê¸°ë°˜ ë¶ˆê³µì • ì•½ê´€ íƒì§€ ê²°ê³¼")
    print("=" * 80)
    print(f"ğŸ“Š ì „ì²´ ìœ„í—˜ë„: {result['overall_risk_score']:.2f} ({result['risk_level']})")
    print(f"ğŸ“ ìš”ì•½: {result['summary']}")
    print()
    
    for i, detection in enumerate(result['detection_results'], 1):
        if detection.analysis.is_unfair:
            print(f"ğŸš¨ ë¶ˆê³µì • ì¡°í•­ {i}:")
            print(f"   ì¡°í•­: {detection.clause_text}")
            print(f"   ìœ„í—˜ë„: {detection.analysis.risk_level} (ì‹ ë¢°ë„: {detection.analysis.confidence:.2f})")
            print(f"   íŒë‹¨ ê·¼ê±°: {detection.analysis.reason}")
            
            if detection.analysis.legal_basis:
                print(f"   ğŸ“š ë²•ì  ê·¼ê±°: {', '.join(detection.analysis.legal_basis)}")
            
            if detection.legal_context:
                print("   ğŸ”— ê´€ë ¨ ë²•ì  ë§¥ë½:")
                for ctx in detection.legal_context[:2]:
                    doc_desc = detector.document_priority[ctx.document_type]["description"]
                    print(f"     - {doc_desc}: {ctx.content[:100]}...")
            
            if detection.analysis.suggestion:
                print(f"   ğŸ’¡ ê°œì„  ì œì•ˆ: {detection.analysis.suggestion}")
            
            print()
    
    # íŒŒì¼ ì¶œë ¥
    if args.output:
        # íƒ€ì„ìŠ¤íƒ¬í”„ í´ë” ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"results/analysis_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"ê²°ê³¼ í´ë” ìƒì„±: {output_dir}")
        
        # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
        json_result = {
            "input_text": result["input_text"],
            "unfair_count": result["unfair_count"],
            "total_clauses": result["total_clauses"],
            "overall_risk_score": result["overall_risk_score"],
            "risk_level": result["risk_level"],
            "summary": result["summary"],
            "analysis_timestamp": timestamp,
            "unfair_clauses": [
                {
                    "clause_text": d.clause_text,
                    "is_unfair": d.analysis.is_unfair,
                    "risk_level": d.analysis.risk_level,
                    "confidence": d.analysis.confidence,
                    "reason": d.analysis.reason,
                    "legal_basis": d.analysis.legal_basis,
                    "suggestion": d.analysis.suggestion,
                    "legal_context": [
                        {
                            "content": ctx.content,
                            "source": ctx.source,
                            "document_type": ctx.document_type,
                            "confidence": ctx.confidence
                        }
                        for ctx in d.legal_context
                    ]
                }
                for d in result["detection_results"] if d.analysis.is_unfair
            ]
        }
        
        # JSON íŒŒì¼ ì €ì¥
        json_file = os.path.join(output_dir, "ai_detection_result.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_result, f, ensure_ascii=False, indent=2)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_file = os.path.join(output_dir, "analysis_report.md")
        _generate_markdown_report(json_result, markdown_file)
        
        print(f"ğŸ“ ê²°ê³¼ê°€ {output_dir} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"   ğŸ“„ JSON: {json_file}")
        print(f"   ğŸ“‹ ë§ˆí¬ë‹¤ìš´: {markdown_file}")

def _generate_markdown_report(result: Dict[str, Any], output_file: str):
    """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""
    
    # ìœ„í—˜ë„ ë ˆë²¨ì— ë”°ë¥¸ ì´ëª¨ì§€
    risk_emojis = {
        "Low": "ğŸŸ¢",
        "Medium": "ğŸŸ¡", 
        "High": "ğŸ”´",
        "Critical": "ğŸš¨"
    }
    
    # ë¬¸ì„œ íƒ€ì…ë³„ ì´ëª¨ì§€
    doc_emojis = {
        "law": "âš–ï¸",
        "standard": "ğŸ“‹",
        "reference": "ğŸ“°"
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# ğŸ›¡ï¸ AI ê¸°ë°˜ ë¶ˆê³µì • ì•½ê´€ íƒì§€ ë³´ê³ ì„œ\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {result['analysis_timestamp']}\n\n")
        
        # ì „ì²´ ìš”ì•½
        f.write(f"## ğŸ“Š ì „ì²´ ìš”ì•½\n\n")
        f.write(f"- **ì´ ì¡°í•­ ìˆ˜**: {result['total_clauses']}ê°œ\n")
        f.write(f"- **ë¶ˆê³µì • ì¡°í•­**: {result['unfair_count']}ê°œ\n")
        f.write(f"- **ì „ì²´ ìœ„í—˜ë„**: {result['overall_risk_score']:.2f}\n")
        f.write(f"- **ìœ„í—˜ ë ˆë²¨**: {risk_emojis.get(result['risk_level'], 'â“')} {result['risk_level']}\n\n")
        f.write(f"**ìš”ì•½**: {result['summary']}\n\n")
        
        # ë¶ˆê³µì • ì¡°í•­ ìƒì„¸ ë¶„ì„
        if result['unfair_clauses']:
            f.write(f"## ğŸš¨ ë¶ˆê³µì • ì¡°í•­ ìƒì„¸ ë¶„ì„\n\n")
            
            for i, clause in enumerate(result['unfair_clauses'], 1):
                f.write(f"### {i}. {risk_emojis.get(clause['risk_level'], 'â“')} ìœ„í—˜ë„: {clause['risk_level']} (ì‹ ë¢°ë„: {clause['confidence']:.2f})\n\n")
                f.write(f"**ì¡°í•­ ë‚´ìš©**:\n```\n{clause['clause_text']}\n```\n\n")
                
                f.write(f"**ë¶ˆê³µì • íŒë‹¨ ê·¼ê±°**:\n{clause['reason']}\n\n")
                
                if clause['legal_basis']:
                    f.write(f"**ğŸ“š ë²•ì  ê·¼ê±°**:\n")
                    for basis in clause['legal_basis']:
                        f.write(f"- {basis}\n")
                    f.write("\n")
                
                if clause['legal_context']:
                    f.write(f"**ğŸ”— ê´€ë ¨ ë²•ì  ë§¥ë½**:\n")
                    for ctx in clause['legal_context']:
                        doc_emoji = doc_emojis.get(ctx['document_type'], 'ğŸ“„')
                        f.write(f"- {doc_emoji} **{ctx['document_type'].upper()}**: {ctx['content'][:100]}...\n")
                        f.write(f"  - ì¶œì²˜: {ctx['source']}\n")
                        f.write(f"  - ì‹ ë¢°ë„: {ctx['confidence']:.2f}\n")
                    f.write("\n")
                
                if clause['suggestion']:
                    f.write(f"**ğŸ’¡ ê°œì„  ì œì•ˆ**:\n{clause['suggestion']}\n\n")
                
                f.write("---\n\n")
        else:
            f.write(f"## âœ… ë¶ˆê³µì • ì¡°í•­ ì—†ìŒ\n\n")
            f.write(f"ë¶„ì„ëœ ëª¨ë“  ì¡°í•­ì´ ê³µì •í•œ ê²ƒìœ¼ë¡œ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n")
        
        # ë¶„ì„ ì •ë³´
        f.write(f"## ğŸ”§ ë¶„ì„ ì •ë³´\n\n")
        f.write(f"- **ë¶„ì„ ëª¨ë¸**: OpenAI GPT-4o-mini\n")
        f.write(f"- **ê²€ìƒ‰ ì‹œìŠ¤í…œ**: Vector RAG + Graph RAG\n")
        f.write(f"- **ë²•ì  ë°ì´í„°ë² ì´ìŠ¤**: ì•½ê´€ ê·œì œë²•, ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²•, í‘œì¤€ì•½ê´€ ë“±\n")
        f.write(f"- **ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"---\n")
        f.write(f"*ì´ ë³´ê³ ì„œëŠ” AI ê¸°ë°˜ ë¶ˆê³µì • ì•½ê´€ íƒì§€ ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*\n")

if __name__ == "__main__":
    main()
